{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "book scraper is designed to target the 53 most popular books in my sample\n",
    "more than 40000 reviews each, or more than 1000000 ratings\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "import pickle\n",
    "import selenium\n",
    "import os\n",
    "import pyprind\n",
    "import multiprocessing as multi\n",
    "import pprint as pp\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Inputs: a Goodreads.com book review url\n",
    "        https://www.goodreads.com/review/show/342857147\n",
    "Returns: a pandas Series containing info about the review and the goodreads user who wrote it.\n",
    "        index = ['url', 'book_title', 'author','reviewtext', 'shelves','review_date', 'review_score', \n",
    "                     'words', 'paragraphs', 'images', 'likes', 'comments',\n",
    "                     'username', 'userurl', 'user_friends', 'user_followers'])\n",
    "if this is changed, update matching dataframe in book_scraper\n",
    "        \n",
    "Process: Use Beautiful Soup to scrape relevant text from a Goodreads.com and the associated\n",
    "        user page. Takes about 5 seconds total.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def review_scraper(url) :\n",
    "    \n",
    "    #initializing some blank variables\n",
    "    book_title = ''\n",
    "    author = ''\n",
    "    soup =''\n",
    "    t0= time.time()\n",
    "    \n",
    "    while len(soup) == 0:\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            page = response.text\n",
    "            soup = BeautifulSoup(page,\"lxml\")\n",
    "        except:\n",
    "            time.sleep(0.25)\n",
    "            print('connection error')\n",
    "            return\n",
    "    \n",
    "    #check for non-existent page\n",
    "    try:\n",
    "        if soup.find_all('a', class_='gr-button')[1].text == 'Back to the Goodreads homepage':\n",
    "            print(url)\n",
    "            return\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        book_title = soup.find('a', class_='bookTitle').text\n",
    "    except:\n",
    "        pass\n",
    "    username =  soup.find('a', class_='userReview').text\n",
    "    author = soup.find('a', class_='authorName').text\n",
    "    review_date = soup.find('span', itemprop=\"publishDate\").text\n",
    "    \n",
    "    \n",
    "    shelves = []\n",
    "    for item in soup.find_all('a', class_='actionLinkLite'):\n",
    "        shelves.append(item.text)\n",
    "\n",
    "    #reviewsoup scrapes the review.  The several try/except loops are necessary\n",
    "    #to deal with reviews that do not have any text\n",
    "    reviewsoup= soup.find('div', class_=\"reviewText mediumText description readable\")\n",
    "    try:\n",
    "        reviewtext = reviewsoup.text\n",
    "    except:\n",
    "        reviewtext = ''\n",
    "    if len(reviewtext) == 0:\n",
    "        print('No Review')\n",
    "        return\n",
    "    \n",
    "    paragraphs = reviewtext.count('\\n')\n",
    "    reviewtext = reviewtext.replace('\\n', ' ').replace('\\t', ' ')\n",
    "    words = reviewtext.count(' ')\n",
    "    try:\n",
    "        images = len(reviewsoup.find_all('img'))\n",
    "    except:\n",
    "        images = 0\n",
    "    try:\n",
    "        likes = int(soup.find('span', class_='likesCount').text.split(' ')[0])\n",
    "    except:\n",
    "        likes = 0\n",
    "    try:\n",
    "        comments = int(soup.find('span', class_='smallText').text.split(' ')[-1].split(')')[0])\n",
    "    except:\n",
    "        comments = 0\n",
    "    try:    \n",
    "        star_text = soup.find('div', class_='rating').text.strip()\n",
    "    except:\n",
    "        star_text = 'not rated'\n",
    "    rating_dict = {'did not like it': 1,\n",
    "                  'it was ok': 2,\n",
    "                  'liked it': 3,\n",
    "                  'really liked it': 4,\n",
    "                  'it was amazing': 5,\n",
    "                  'not rated': 0}\n",
    "    try:\n",
    "        review_score = rating_dict[star_text]\n",
    "    except:\n",
    "        review_score = star_text\n",
    "        \n",
    "\n",
    "    #usersoup looks at the user, and in particular how many friends and followers they have\n",
    "    userurl =  'https://www.goodreads.com'+ soup.find('a', class_='userReview').get('href')    \n",
    "    usersoup = BeautifulSoup(requests.get(userurl).text, 'lxml')\n",
    "    user_followers= 0\n",
    "    user_friends = 0\n",
    "    for item in usersoup.find_all('a', rel='nofollow'):\n",
    "        if 'Friends' in item.text:\n",
    "            user_friends = item.text.split(' ')[-1].replace(',','').strip('()')\n",
    "            if user_friends == 'Friends':\n",
    "                user_friends = 0\n",
    "        if 'people are' in item.text:\n",
    "            user_followers = int(item.text.split(' ')[0])\n",
    "    \n",
    "    t1 = time.time()\n",
    "    #print('Time %.3f :' % (t1-t0), book_title)\n",
    "    return(pd.Series([url, book_title, author, reviewtext, shelves, review_date, review_score, words, paragraphs, images, likes, comments,\n",
    "           username, userurl, user_friends, user_followers],\n",
    "                    index = ['url', 'book_title', 'author','reviewtext', 'shelves','review_date', 'review_score', \n",
    "                                 'words', 'paragraphs', 'images', 'likes', 'comments',\n",
    "                                 'username', 'userurl', 'user_friends', 'user_followers']))\n",
    "\n",
    "#, times_reviewed, times_rated, average_rating, isbn, book_date, book_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "input: driver, counter\n",
    "returns: list of book_urls\n",
    "\n",
    "        sel_scrape is a helper function used to grab book review links using Selenium.\n",
    "        The page numbers at the bottom of a book page are dynamic Ajak objects, not hyperlinks\n",
    "        next_find locates the 'next' button on the page\n",
    "        review_finder returns 30 links\n",
    "        the while loop below gives the page 20 seconds to fully load, and then scrapes the rest\n",
    "        if next.click() doesn't do anything, the function returns\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "def sel_scrape(driver, i):\n",
    "\n",
    "    def next_find(driver):\n",
    "        element = driver.find_element_by_link_text('next Â»')\n",
    "        if element:\n",
    "            return element\n",
    "\n",
    "    def review_finder(driver):\n",
    "        element = driver.find_elements_by_link_text('see review')\n",
    "        if len(element)==30:\n",
    "            return element\n",
    "    \n",
    "    t_end = time.time() + 20\n",
    "    while time.time() < t_end:\n",
    "        links = review_finder(driver)\n",
    "        if links == False:\n",
    "            links = driver.find_elements_by_link_text('see review')\n",
    "        new_urls = []\n",
    "        try:\n",
    "            for item in links:\n",
    "                new_urls.append(item.get_attribute('href').split('?')[0])\n",
    "                print(i, end=' ')\n",
    "                i = i +1\n",
    "        except:\n",
    "            pass\n",
    "        if len(new_urls)==30:\n",
    "            print('')\n",
    "            break\n",
    "    try:\n",
    "        next_find(driver).click()\n",
    "        time.sleep(3)\n",
    "        driver.find_element_by_tag_name('body').send_keys(Keys.CONTROL + Keys.HOME)\n",
    "    except:\n",
    "        return (new_urls, 'END')\n",
    "    return (new_urls, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Inputs: A Goodreads book URL\n",
    "        https://www.goodreads.com/book/show/11324722-the-righteous-mind\n",
    "        pages: how many pages to scrape, an int 1-10\n",
    "        \n",
    "Returns: save a dataframe of review and book info to disk, as {Book Title} book.pkl\n",
    "        if there is an error, return the book_url for re-scraping at a later date.\n",
    "        Prints the book_url, number of reviews, and total time to terminal.\n",
    "\n",
    "        \n",
    "Process:  grab the first 30 reviews using Beautiful Soup.  \n",
    "        Selenium is required to view the next pages of reviews.  By default, \n",
    "        this grabs a total of 90 reviews.  Reviews added to review_urls, a list which\n",
    "        is processed using the multiprocessing.map function and review_scraper.  Series\n",
    "        from review_scraper are assembled into a pandas DataFrame and pickled to disk.\n",
    "        \n",
    "        Multiprocessing gives an increase in speed equal to number of CPU cores.  \n",
    "        On my laptop and with decent internet, scraping 300 reviews takes about 400 seconds.\n",
    "        The function checks the duration between beginning and end.\n",
    "\"\"\"        \n",
    "\n",
    "\n",
    "def book_scraper(book_url, pages):\n",
    "    t2 = time.time()\n",
    "    booksoup = BeautifulSoup(requests.get(book_url).text, 'lxml')\n",
    "    print(book_url)\n",
    "\n",
    "    #first we grab all book review links visible on the page\n",
    "    review_urls = []\n",
    "    i = 0\n",
    "            \n",
    "    #we need to use Selenium to get reviews on pages 2 and 3\n",
    "    chromedriver = \"/usr/lib/chromium-browser/chromedriver\" # path to the chromedriver executable\n",
    "    os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "\n",
    "    driver = webdriver.Chrome(chromedriver)\n",
    "    driver.get(book_url)\n",
    "    \n",
    "    for j in range(10):\n",
    "        new_urls, i= sel_scrape(driver, i)\n",
    "        review_urls = review_urls + new_urls\n",
    "        if i == 'END':\n",
    "            break\n",
    "    driver.close()\n",
    "    \n",
    "    #quick filter to remove some none-review things Selenium throws in the list\n",
    "    #and yes, there is a cool lambda function way to do this.  \n",
    "    #and no, I didn't implement it past midnight Sunday\n",
    "    filtered = []\n",
    "    for item in review_urls:\n",
    "        if 'https:' in item:\n",
    "            filtered.append(item)\n",
    "        \n",
    "    review_urls = filtered\n",
    "    print('aiming for %d reviews' % len(review_urls))\n",
    "\n",
    "    #booksoup returns information about the book\n",
    "    times_rated = int(booksoup.find('span', class_='votes value-title').text.strip().replace(',',''))\n",
    "    times_reviewed = int(booksoup.find('span', class_='count value-title').text.strip().replace(',',''))\n",
    "    average_rating = float(booksoup.find('span', class_='average').text)\n",
    "    isbn =''\n",
    "    for item in booksoup.find_all('div', class_='infoBoxRowItem'):\n",
    "        try:\n",
    "            a = item.text.strip().split('\\n')[0]\n",
    "            b = re.search(r'\\w\\w\\w\\w\\w\\w\\w\\w\\w\\w', a)\n",
    "            if b != None:\n",
    "                isbn = b[0]\n",
    "        except: \n",
    "            pass \n",
    "        for item in booksoup.find_all('div', class_='row'):\n",
    "            if 'Published' in item.text:\n",
    "                a = item.text\n",
    "                book_date = a.strip().split('\\n')[1].strip()\n",
    "\n",
    "    #df is a dataframe of book info scraped from 90 reviews\n",
    "    #we initialize it with the columns that will be passed by review_scraper\n",
    "    #make sure to update this and review_scraper output in sync\n",
    "    df = pd.DataFrame(columns= ['url', 'book_title', 'author', 'reviewtext', 'shelves', 'review_date', 'review_score', \n",
    "                                 'words', 'paragraphs', 'images', 'likes', 'comments',\n",
    "                                 'username', 'userurl', 'user_friends', 'user_followers'])\n",
    "    \n",
    "    times = []\n",
    "    i = 1\n",
    "    \n",
    "    #we call review_scraper for every url on the page, passing it info about the book\n",
    "    #pool is the most elegant way to multithread\n",
    "    \n",
    "    reviews = []\n",
    "    try:\n",
    "        pool = multi.Pool()\n",
    "        reviews = pool.map(review_scraper, review_urls)\n",
    "        pool.terminate()\n",
    "    except:\n",
    "        print('SCRAPE ERROR', book_url)\n",
    "        return (book_url)\n",
    "\n",
    "    \n",
    "    filtered = []\n",
    "    for item in reviews:\n",
    "        if str(type(item)) != \"<class 'NoneType'>\":\n",
    "            filtered.append(item)\n",
    "        \n",
    "    reviews = filtered\n",
    "    \n",
    "    t3 = time.time()\n",
    "    print('REVIEWS: ', len(reviews), 'TOTAL TIME: ', t3-t2)\n",
    "    \n",
    "    #create a dataframe from the reviews, and add on book info\n",
    "    df = pd.DataFrame.from_records(reviews)\n",
    "    df['times_reviewed'] = times_reviewed\n",
    "    df['times_rated'] =  times_rated\n",
    "    df['average_rating'] =average_rating\n",
    "    df['isbn'] = isbn\n",
    "    df['book_date'] = book_date\n",
    "    df['book_url']= book_url    \n",
    "    \n",
    "    #remove restricted characters from file name\n",
    "    try:\n",
    "        book_title = df.book_title[0]\n",
    "    except:\n",
    "        book_title = 'Bad Title'\n",
    "    book_title = re.sub('\\/|\\\\|\\?|\\%|\\*|\\:|\\<|\\>|\\.|\\\"|\\|', \"\", book_title)\n",
    "    \n",
    "    #and save as {title} book.pkl\n",
    "    filename = book_title+' book.pkl'\n",
    "    outfile = open(filename,'wb')\n",
    "    pickle.dump(df,outfile)\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Goodreads.com list represent large collections of books liked by the community. \n",
    "For this prototype, I've left this code outside a function.  It takes a page of a list\n",
    "(100 books) and adds the book urls to a set, which prevents duplicate scrapes\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "list_urls = ['https://www.goodreads.com/list/show/6.Best_Books_of_the_20th_Century',\n",
    "            'https://www.goodreads.com/list/show/824.Best_Non_fiction_War_Books',\n",
    "            'https://www.goodreads.com/list/show/134.Best_Non_Fiction_no_biographies_',\n",
    "            'https://www.goodreads.com/list/show/3.Best_Science_Fiction_Fantasy_Books',\n",
    "            'https://www.goodreads.com/list/show/397.Best_Paranormal_Romance_Series',\n",
    "            'https://www.goodreads.com/list/show/348.Thrillers',\n",
    "            'https://www.goodreads.com/list/show/5490.YA_Books_Far_Better_than_Twilight',\n",
    "            'https://www.goodreads.com/list/show/29013.Best_Biographies',]\n",
    "\n",
    "list_urls2 = ['https://www.goodreads.com/list/show/10942.Our_Favorite_Indie_Reads',\n",
    "             'https://www.goodreads.com/list/show/16.Best_Books_of_the_19th_Century',\n",
    "             'https://www.goodreads.com/list/show/1083.The_Most_Influential_Books_in_History',\n",
    "             'https://www.goodreads.com/list/show/397.Best_Paranormal_Romance_Series?page=2',\n",
    "             'https://www.goodreads.com/list/show/281.Best_Memoir_Biography_Autobiography',\n",
    "             'https://www.goodreads.com/list/show/3.Best_Science_Fiction_Fantasy_Books?page=2',\n",
    "             'https://www.goodreads.com/list/show/3.Best_Science_Fiction_Fantasy_Books?page=3',\n",
    "             'https://www.goodreads.com/list/show/824.Best_Non_fiction_War_Books?page=2',\n",
    "             'https://www.goodreads.com/list/show/6.Best_Books_of_the_20th_Century?page=2',\n",
    "             'https://www.goodreads.com/list/show/952.1001_Books_You_Must_Read_Before_You_Die',\n",
    "             'https://www.goodreads.com/list/show/952.1001_Books_You_Must_Read_Before_You_Die?page=2',\n",
    "             'https://www.goodreads.com/list/show/134.Best_Non_Fiction_no_biographies_?page=2']\n",
    "\n",
    "book_urls = set()\n",
    "\n",
    "for thing in [list_urls2[5]]:\n",
    "    listsoup = BeautifulSoup(requests.get(thing).text, 'lxml')\n",
    "    for item in listsoup.find_all('a', class_='bookTitle'):\n",
    "        book_urls.add('https://www.goodreads.com'+item.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load books already scraped\n",
    "\n",
    "jar = []\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for file in os.listdir():\n",
    "    if file.split(' ')[-1]=='book.pkl':\n",
    "        jar.append(file)\n",
    "\n",
    "jar.append('Benson_Data_Sample.pkl')        \n",
    "\n",
    "for file in jar:\n",
    "    d = pickle.load(open(file, \"rb\" ))\n",
    "    df = pd.concat([df,d], sort=False, ignore_index=True)\n",
    "\n",
    "books_done = set(df.book_url.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example target\n",
    "#note if cell output is zero, either change the url below to something not yet scraped\n",
    "#or rerun the notebook and skip the cell above\n",
    "targets = set(['https://www.goodreads.com/book/show/37976541-bad-blood'])\n",
    "\n",
    "#alternatively, if you have several days\n",
    "#targets = book_urls\n",
    "\n",
    "errors =set()\n",
    "targets = (targets-books_done).union(errors)-{None}\n",
    "print('targets:', len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.goodreads.com/book/show/37976541-bad-blood\n",
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 \n",
      "30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 \n",
      "90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 \n",
      "120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 \n",
      "150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 \n",
      "180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 \n",
      "210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 \n",
      "240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 \n",
      "270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 \n",
      "aiming for 300 reviews\n",
      "REVIEWS:  300 TOTAL TIME:  461.0150969028473\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This cell does the actual scraping\n",
    "\"\"\"\n",
    "errors = set()\n",
    "\n",
    "for book in targets:\n",
    "    errors.add(book_scraper(book, 10))\n",
    "    books_done.add(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'https://www.goodreads.com/book/show/100365.The_Mote_in_God_s_Eye',\n",
       " 'https://www.goodreads.com/book/show/108942.Bad_Luck_and_Trouble',\n",
       " 'https://www.goodreads.com/book/show/13831.Alanna',\n",
       " 'https://www.goodreads.com/book/show/32067.Lady_Chatterley_s_Lover',\n",
       " 'https://www.goodreads.com/book/show/32530.The_Third_Victim',\n",
       " 'https://www.goodreads.com/book/show/350.Stranger_in_a_Strange_Land',\n",
       " 'https://www.goodreads.com/book/show/45032.Mansfield_Park',\n",
       " 'https://www.goodreads.com/book/show/6487308-fallen'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{None, 'https://www.goodreads.com/book/show/45032.Mansfield_Park'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
