{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Linear Regression Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classical Assumptions of Ordinary Least Squares\n",
    "\n",
    "1. Regression is linear in parameters & correctly specified\n",
    "2. The error terms are normally distributed and zero population mean\n",
    "3. The error term has constant variance $Var({\\epsilon_i})={\\sigma^2}$ for every i (no heteroskedasticity)\n",
    "4. Errors are uncorrelated across observations: $cov({\\epsilon_i},{\\epsilon_j})=0$ for two observations i and j (no serial correlation)\n",
    "5. No independent variable is a perfect linear function of any other independent variable (no perfect multi-collinearity)\n",
    "\n",
    "We will use this [Duke Resource](http://people.duke.edu/~rnau/testing.htm) as a guide for the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Python 2 & 3 Compatibility\n",
    "from __future__ import print_function, division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import patsy\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy.stats as stats\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumption #1 : Regression is linear in parameters & correctly specified\n",
    "\n",
    "This is linear: $ Y= {\\beta_0}+ {\\beta_1}X_1+{\\beta_2}X_2 +{\\epsilon}$\n",
    "This is not: $ Y= {\\beta_0}+ e^{\\beta_1}X^{\\beta_2}$\n",
    "\n",
    "Notice we're not talking about straight lines vs. curved. Rather, the second equation doesn't have scalar coefficients; they change X in strange ways. Also, we can't use linear algebra (matrix multiplication) to solve our model for one optimal solution. These 'non-linear models' are a whole different domain of modeling, and outside the scope of this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def diagnostic_plot(x, y):\n",
    "    plt.figure(figsize=(20,5))\n",
    "    \n",
    "    rgr = LinearRegression()\n",
    "    rgr.fit(x.reshape(s,1),y)\n",
    "    pred = rgr.predict(x.reshape(s,1))\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.scatter(x,y)\n",
    "    plt.plot(x, pred, color='blue',linewidth=1)\n",
    "    plt.title(\"Regression fit\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    res = y - pred\n",
    "    plt.scatter(pred, res)\n",
    "    plt.title(\"Residual plot\")\n",
    "    plt.xlabel(\"prediction\")\n",
    "    plt.ylabel(\"residuals\")\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    #Generates a probability plot of sample data against the quantiles of a \n",
    "    # specified theoretical distribution \n",
    "    stats.probplot(res, dist=\"norm\", plot=plt)\n",
    "    plt.title(\"Normal Q-Q plot\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate data\n",
    "s = 1000\n",
    "x = np.random.uniform(low=-5, high=5, size=s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep = 2*np.random.randn(s)\n",
    "beta = 2\n",
    "y = beta*x + ep\n",
    "\n",
    "diagnostic_plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diagnose: (1) Inspect plot of observed data vs predicted values (points should be symmetrically about the line)    OR (2) Inspect your residuals (points should by  symmetric about (y=0) and roughly constant variance)\n",
    "\n",
    "Look carefully for evidence of a \"bowed\" pattern, indicating that the model makes systematic errors whenever it is making unusually large or small predictions, as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep = 3*np.random.randn(s)\n",
    "beta1 = 1\n",
    "y = beta1*(x**2) + ep\n",
    "\n",
    "diagnostic_plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have to add bike data here. . \n",
    "# Let's explore our assumptions - Let's look at some bike data\n",
    "data = pd.read_csv('../../week02-luther1/03-regression_statsmodels/data/hour.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# removing non-numeric data\n",
    "cols = ['season', 'yr', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit', 'temp', 'hum', 'windspeed']\n",
    "X = data[cols]\n",
    "y = data.cnt   # predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# develop OLS with Sklearn\n",
    "lr = LinearRegression()\n",
    "fit = lr.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumption #2 : Residuals ( ${e_i} = Y_i-\\hat{Y}_i$ ) should be normally distributed with zero mean: \n",
    "\n",
    "We can check this assumption as follows by plotting our residuals vs $\\hat{Y}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot your predicted values on the x-axis, and your residuals on the y-axis\n",
    "\n",
    "data['predict']=fit.predict(X)\n",
    "data['resid']=data.cnt-data.predict\n",
    "with sns.axes_style('white'):\n",
    "    plot=data.plot(kind='scatter',\n",
    "                  x='predict',y='resid',alpha=0.2,figsize=(10,6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Q: What is going on with the lower bound of the plot? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Inspect histogram)\n",
    "data.cnt.hist(bins=35)\n",
    "plt.title('Histogram of Dependent Variable (User Counts)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can diagnose/ inspect our residual normality assumption using qqplot:\n",
    "stats.probplot(data['resid'], dist=\"norm\", plot=plt)\n",
    "plt.title(\"Normal Q-Q plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A : This is count data!   the assumption that errors are normally distributed \n",
    "# cant be held (more about count data when we discuss the poisson distribution in a \n",
    "# couple of weeks).  Remember that our dependent variable should not be categorical or \n",
    "# ordinal (i.e. rank of films ) either."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumption #3: The error term must have constant variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's bring in movie data\n",
    "movie=pd.read_csv('../../../challenges/challenges_data/2013_movies.csv')\n",
    "\n",
    "# let's just drop nan's for now..\n",
    "# Incidentally, how should we handle our NaNs ?\n",
    "movie=movie.dropna()\n",
    "movie.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at numeric data\n",
    "X = movie[['Budget','Runtime']]\n",
    "y = movie.DomesticTotalGross\n",
    "\n",
    "model = sm.OLS(y,X)\n",
    "fit = model.fit()\n",
    "fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual plot: plot residuals vs predicted\n",
    "movie['predict']=fit.predict(X)\n",
    "movie['resid']= y-movie.predict\n",
    "with sns.axes_style('white'):\n",
    "    plot = movie.plot(\n",
    "        kind='scatter', x='predict', y='resid', alpha=0.5, figsize=(10,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Q: What is wrong with the above plot? Which assumption is this plot inconsistent with? \n",
    "   What can we do about it?  \n",
    "A: This plot is inconsistent with **Assumption #3**: (The error term must have constant variance). Here we see signs of heteroskedasticity.\n",
    "However, the rule of thumb is: OLS regression isn't too impacted by heteroscedasticity as long as the maximum variance is not greater than 4 times the minimum variance (as in this case).  If the residual variance of your model exceeds this range, we can opt for a Weighted Least Squares model: http://statsmodels.sourceforge.net/0.6.0/examples/notebooks/generated/wls.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive skew \n",
    "movie.DomesticTotalGross.hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick reg plot\n",
    "\n",
    "plt.scatter(movie.Budget,y)\n",
    "plt.scatter(movie.Budget,movie.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try log transformation ?\n",
    "# not great !\n",
    "np.log(movie.DomesticTotalGross).hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try another transformation!\n",
    "Examples of [box-cox in action](http://scikit-learn.org/dev/auto_examples/preprocessing/plot_power_transformer.html) (sklearn calls it power transform) on various distributions.\n",
    "\n",
    "Note the [scipy transform formula](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.boxcox.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transforming both the x & y can assist the fit of your model\n",
    "# box-cox : find a suitable power (lamba)\n",
    "# lambda is unknown, and can be discovered via the MLE approach\n",
    "\n",
    "# The power transformed Y are assumed to be distributed normally with \n",
    "# constant variance, and we set up the liklihood function under these conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "lamb=stats.boxcox_normmax(movie.DomesticTotalGross, brack=(-1.9, 1.9))\n",
    "print(lamb)\n",
    "y_t=(np.power(movie.DomesticTotalGross,-0.2282)-1)/-0.2282\n",
    "\n",
    "plt.hist(y_t);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot to show optimal lambda values\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "prob = stats.boxcox_normplot(movie.DomesticTotalGross, -3, 5, plot=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On your own: \n",
    "Use box-cox transformation to also transform Budget & Runtime \n",
    "Visualize residuals on a new model with all features transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fix:**  Try a log transformation or Box Cox Transformation. \n",
    "It may help !  but not always.  \n",
    "\n",
    "Sometimes we can observe inconstant variance due to time series effects  (ex: we could expect this to happen if we didnt account for inflation for Budget/DG data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumption 4: Errors are uncorrelated across observations\n",
    "\n",
    "To check this assumption, let's plot residuals vs. time: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the date and time fields into a single datetime column\n",
    "movie[\"DATE_TIME\"] = pd.to_datetime(movie.ReleaseDate , format=\"%Y-%m-%d\")\n",
    "movie.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = movie[['DATE_TIME','resid']].set_index('DATE_TIME')\n",
    "ts.plot(style=\".\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix: If we did see a strong relationship between errors and time,this would indicate that our model is incorrectly specified. We'd want to apply a time series model instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumption #5: No independent variable is a perfect linear function of any other independent variable (no perfect multi-collinearity)\n",
    "\n",
    "As discussed: \n",
    "1. Inspect correlations of independent features\n",
    "2. Keep an eye on condition number!\n",
    "3. Consider Partial Least Squares or projection into latent space (PCA)\n",
    "4. Use Ridge regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "- [Duke Guide](http://people.duke.edu/~rnau/testing.htm)\n",
    "- [10 Assumptions List](http://r-statistics.co/Assumptions-of-Linear-Regression.html)\n",
    "- [University of Wisconsin List](http://blog.uwgb.edu/bansalg/statistics-data-analytics/linear-regression/what-are-the-four-assumptions-of-linear-regression/)\n",
    "- [Another Guide](http://www.statisticssolutions.com/assumptions-of-linear-regression/)\n",
    "\n",
    ">Note: You'll see lists including 4, 5, or more assumptions. It's not black and white, and some sources split one major assumption into smaller groups."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": false,
   "nav_menu": {
    "height": "156px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": false,
   "threshold": "3",
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
